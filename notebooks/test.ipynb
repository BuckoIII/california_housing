{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfdde950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this scripts contains helper functions for run.py\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import sklearn.datasets\n",
    "\n",
    "import math, copy\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# define data cleaning fucns\n",
    "##############################################################################################################\n",
    "\n",
    "def load_data():\n",
    "    housing = sklearn.datasets.fetch_california_housing()\n",
    "\n",
    "    df = pd.DataFrame(data=np.c_[housing['data'], housing['target']],\n",
    "                      columns=housing['feature_names'] + ['target'])\n",
    "    df = df.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_test_split(df):\n",
    "    n = len(df)\n",
    "\n",
    "    # train test split (2/3 train, 1/3 test)\n",
    "    n_train = round(2 / 3 * n)\n",
    "\n",
    "    train_df = df[:n_train]\n",
    "    test_df = df[n_train:]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def initial_rand(X):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "    w = np.random.randn(n).reshape(n, 1) * 0.01\n",
    "    b = np.random.randint(0, 100) * 0.01\n",
    "\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def initial_zeros(X):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # m = number of training examples\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # n = number of features\n",
    "    n = X.shape[1]\n",
    "\n",
    "    w = np.zeros(n).reshape(n, 1).T\n",
    "    b = 0\n",
    "\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def set_train_vars(X_df):\n",
    "    # m = number of training examples\n",
    "    m = X_df.values.shape[0]\n",
    "\n",
    "    # n = number of features\n",
    "    n = len(X_df.drop(columns='target').columns)\n",
    "\n",
    "    # X should be a matrix of with m (number training examples) rows and n (number features) columns\n",
    "    X = X_df.drop(columns='target').values.reshape(m, n)\n",
    "\n",
    "    # Y should be a matrix with 1 row and n columns\n",
    "    Y = X_df['target'].values.reshape(1, m)\n",
    "\n",
    "    return X, Y, m, n\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# define gradient descent functions\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "def forward_prop(X, w, b):\n",
    "    n = X.shape[0]\n",
    "    # reshape step important for later functions\n",
    "    Y_hat = np.dot(w, X.T) + b\n",
    "\n",
    "    return Y_hat\n",
    "\n",
    "\n",
    "def calculate_cost(X, Y, w, b):\n",
    "    m = X.shape[0]\n",
    "    Y_hat = forward_prop(X, w, b)\n",
    "    cost = np.sum((Y_hat - Y) ** 2) / (2 * m)\n",
    "    return cost\n",
    "\n",
    "\n",
    "def calculate_grads(X, Y, w, b):\n",
    "    m, n = X.shape\n",
    "    Y_hat = forward_prop(X, w, b)\n",
    "    db = np.mean(Y_hat - Y)\n",
    "    dw = np.sum(((Y_hat - Y) * X.T), axis=1) / m\n",
    "    return db, dw\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters\n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters\n",
    "      b (scalar)       : Updated value of parameter\n",
    "      \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  # avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)  ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw  ##None\n",
    "        b = b - alpha * dj_db  ##None\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "    end = time.time()\n",
    "    train_time = end - start\n",
    "\n",
    "    return w, b, J_history, train_time  # return final w,b and J history for graphing\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "# testing funcs\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "def set_test_vars(X_df):\n",
    "    m, n = X_df.shape\n",
    "    X_test = X_df.drop(columns=['target']).values\n",
    "    Y_test = X_df['target'].values\n",
    "\n",
    "    return X_test, Y_test, m, n\n",
    "\n",
    "\n",
    "def predict(X_in, w_arr, b_val):\n",
    "    Y_hat = np.dot(w_arr, X_in.T) + b_val\n",
    "    return Y_hat\n",
    "\n",
    "\n",
    "def rmse(Y, Y_hat):\n",
    "    return np.sqrt(np.mean((Y_hat - Y) ** 2))\n",
    "\n",
    "\n",
    "def mape(Y, Y_hat):\n",
    "    return np.mean(abs((Y - Y_hat) / Y_hat))\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# def feature engineering funcs\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "def scale_features(X):\n",
    "    max_features = np.array([np.max(X.T[:][i]) for i in range(X.shape[1])])\n",
    "    scaled_features = np.array([X.T[:][0] / max_features[i] for i in range(X.shape[1])]).T\n",
    "\n",
    "    return max_features, scaled_features\n",
    "\n",
    "def set_scaled_vars(train_df):\n",
    "    X, Y, m, n = set_train_vars(train_df)\n",
    "    X_max_features, X = scale_features(X)\n",
    "    return X, Y, m, n\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# def utils\n",
    "##############################################################################################################\n",
    "\n",
    "def results_exists():\n",
    "    results_path = Path(Path.cwd().parent.absolute(), 'data', 'results.csv')\n",
    "    return results_path.is_file()\n",
    "\n",
    "def last_results_update():\n",
    "    results_path = Path(Path.cwd().parent.absolute(), 'data', 'results.csv')\n",
    "    df = pd.read_csv(results_path)\n",
    "    last_updated = df.last_updated.max()\n",
    "    past_models = df.model_name.values\n",
    "    return last_updated, past_models\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7e78c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     2.63   \n",
      "Iteration  150: Cost     0.97   \n",
      "Iteration  300: Cost     0.64   \n",
      "Iteration  450: Cost     0.57   \n",
      "Iteration  600: Cost     0.55   \n",
      "Iteration  750: Cost     0.54   \n",
      "Iteration  900: Cost     0.54   \n",
      "Iteration 1050: Cost     0.53   \n",
      "Iteration 1200: Cost     0.52   \n",
      "Iteration 1350: Cost     0.52   \n",
      "b,w found by gradient descent: [[1.06867797e+00 3.08274546e-01 1.12961589e-01 4.70556059e-01\n",
      "  5.61166296e-04 2.67298559e-02]],1.6804103172049092\n",
      "\n",
      "mape: 0.8656026957960247 rmse: 14.563371229395923\n",
      "\n",
      "should return:\n",
      "mape 0.38216067257255476 rmse 1.0399675929681738\n"
     ]
    }
   ],
   "source": [
    "# test notebook script\n",
    "\n",
    "df = load_data()\n",
    "train_df, test_df = train_test_split(df)\n",
    "\n",
    "# set vars\n",
    "# X, Y, m, n = set_train_vars(train_df)\n",
    "# X_max_features, X = scale_features(X)\n",
    "X, Y, m, n = set_scaled_vars(train_df)\n",
    "# calculate model weights with functions\n",
    "\n",
    "w, b = initial_zeros(X)\n",
    "\n",
    "dw = np.zeros_like(w)\n",
    "db = np.array([0.])\n",
    "\n",
    "cost_history = []\n",
    "num_iters = 1500\n",
    "learning_rate = 5*10**-3\n",
    "# run gradient descent \n",
    "w_final, b_final, cost_history, train_time = gradient_descent(X, Y, w, b,\n",
    "                                calculate_cost, calculate_grads,\n",
    "                                learning_rate, num_iters)\n",
    "\n",
    "\n",
    "# set test vars\n",
    "X, Y, m, n = set_test_vars(test_df)\n",
    "\n",
    "# predict\n",
    "Y_hat = predict(X, w_final, b_final)\n",
    "\n",
    "print(f\"b,w found by gradient descent: {w_final},{b_final}\")\n",
    "m,_ = X.shape\n",
    "\n",
    "\n",
    "print('\\nmape:',mape(Y, Y_hat),'rmse:', rmse(Y, Y_hat))\n",
    "print('\\nshould return:\\nmape 0.38216067257255476 rmse 1.0399675929681738')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db021709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     2.63   \n",
      "Iteration  150: Cost     0.97   \n",
      "Iteration  300: Cost     0.64   \n",
      "Iteration  450: Cost     0.57   \n",
      "Iteration  600: Cost     0.55   \n",
      "Iteration  750: Cost     0.54   \n",
      "Iteration  900: Cost     0.54   \n",
      "Iteration 1050: Cost     0.53   \n",
      "Iteration 1200: Cost     0.52   \n",
      "Iteration 1350: Cost     0.52   \n",
      "\n",
      "b,w found by gradient descent: 1.6804103172049092,[[1.06867797e+00 3.08274546e-01 1.12961589e-01 4.70556059e-01\n",
      "  5.61166296e-04 2.67298559e-02]] \n",
      "\n",
      "mape: 0.38216067257255476 rmse: 1.0399675929681738\n",
      "\n",
      "should return:\n",
      "mape 0.38216067257255476 rmse 1.0399675929681738\n"
     ]
    }
   ],
   "source": [
    "# test run sclaed features using run.py logic\n",
    "\n",
    "cost_func = set_scaled_vars\n",
    "\n",
    "df = load_data()\n",
    "train_df, test_df = train_test_split(df)\n",
    "\n",
    "\n",
    "cost_history = []\n",
    "num_iters = 1500\n",
    "learning_rate = 5*10**-3\n",
    "\n",
    "# set train vars\n",
    "X, Y, m, n = cost_func(train_df)\n",
    "\n",
    "w, b = initial_zeros(X)\n",
    "\n",
    "# run gradient descent\n",
    "w_final, b_final, cost_history, train_time = gradient_descent(X, Y, w, b,\n",
    "                                            calculate_cost, calculate_grads,\n",
    "                                            learning_rate, num_iters)\n",
    "print(f\"\\nb,w found by gradient descent: {b_final},{w_final} \")\n",
    "\n",
    "# set test vars\n",
    "X, Y, m, n = set_test_vars(test_df)\n",
    "\n",
    "# predict\n",
    "Y_hat = predict(scale_features(X)[1], w_final, b_final)\n",
    "\n",
    "print('\\nmape:',mape(Y, Y_hat),'rmse:', rmse(Y, Y_hat))\n",
    "print('\\nshould return:\\nmape 0.38216067257255476 rmse 1.0399675929681738')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063e777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c39df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
