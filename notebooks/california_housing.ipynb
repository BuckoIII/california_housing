{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9ba262",
   "metadata": {},
   "source": [
    "Linear regression using\n",
    "\n",
    "Boston Housing Data\n",
    "\n",
    "dataset docs: https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fa6f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "import math, copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13b00d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n",
       " 'frame': None,\n",
       " 'target_names': ['MedHouseVal'],\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block group\\n        - HouseAge      median house age in block group\\n        - AveRooms      average number of rooms per household\\n        - AveBedrms     average number of bedrooms per household\\n        - Population    block group population\\n        - AveOccup      average number of household members\\n        - Latitude      block group latitude\\n        - Longitude     block group longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nAn household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surpinsingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "88878c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.random.seed(1)\n",
    "np.random.randint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5defc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers - data clean\n",
    "\n",
    "def load_data():\n",
    "    housing  = fetch_california_housing()\n",
    "    \n",
    "    df = pd.DataFrame(data= np.c_[housing['data'], housing['target']],\n",
    "                     columns= housing['feature_names'] + ['target'])\n",
    "    return df\n",
    "    \n",
    "\n",
    "def train_test_split(df):\n",
    "    n = len(df)\n",
    "    \n",
    "    # train test split (2/3 train, 1/3 test)\n",
    "    n_train = round(2/3*n)\n",
    "\n",
    "    train_df = df[:n_train]\n",
    "    test_df = df[n_train:]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da8fadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers - single variable linear regression\n",
    "# my version that was coded from scratch\n",
    "\n",
    "def df_to_input(df):\n",
    "    m = len(df)\n",
    "    X = df['AveBedrms'].values.reshape(1, m)\n",
    "    Y = df['target'].values.reshape(1, m)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def initial_rand(X):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    w = np.random.randn(n).reshape(n,1) * 0.01\n",
    "    b = np.random.randint(0,100) * 0.01 \n",
    "    \n",
    "    return w, b\n",
    "    \n",
    "    \n",
    "def single_var_backprop(X, Y, w_in, b_in, Y_hat, learning_rate = 0.005, num_iters = 10000):\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = copy.deepcopy(b_in)  \n",
    "    \n",
    "    for i in range (num_iters):\n",
    "        db, dw = calc_grads(X, Y, Y_hat)\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append(compute_cost(X, Y_hat, Y))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec05eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "train_df, test_df = train_test_split(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f51aa1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13755</th>\n",
       "      <td>2.8125</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.099515</td>\n",
       "      <td>1.135922</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>4.847087</td>\n",
       "      <td>34.06</td>\n",
       "      <td>-117.16</td>\n",
       "      <td>1.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13756</th>\n",
       "      <td>1.8152</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.223660</td>\n",
       "      <td>1.024030</td>\n",
       "      <td>1412.0</td>\n",
       "      <td>2.609982</td>\n",
       "      <td>34.06</td>\n",
       "      <td>-117.16</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13757</th>\n",
       "      <td>7.2972</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.471344</td>\n",
       "      <td>1.038538</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>2.874506</td>\n",
       "      <td>34.01</td>\n",
       "      <td>-117.14</td>\n",
       "      <td>2.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13758</th>\n",
       "      <td>6.5648</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.355844</td>\n",
       "      <td>1.020779</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>2.683117</td>\n",
       "      <td>34.03</td>\n",
       "      <td>-117.15</td>\n",
       "      <td>2.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13759</th>\n",
       "      <td>6.1461</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.847633</td>\n",
       "      <td>1.036982</td>\n",
       "      <td>1818.0</td>\n",
       "      <td>2.689349</td>\n",
       "      <td>34.03</td>\n",
       "      <td>-117.15</td>\n",
       "      <td>2.171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13760 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "13755  2.8125      33.0  5.099515   1.135922      1997.0  4.847087     34.06   \n",
       "13756  1.8152      17.0  4.223660   1.024030      1412.0  2.609982     34.06   \n",
       "13757  7.2972      26.0  7.471344   1.038538      2909.0  2.874506     34.01   \n",
       "13758  6.5648      32.0  7.355844   1.020779      1033.0  2.683117     34.03   \n",
       "13759  6.1461      26.0  7.847633   1.036982      1818.0  2.689349     34.03   \n",
       "\n",
       "       Longitude  target  \n",
       "0        -122.23   4.526  \n",
       "1        -122.22   3.585  \n",
       "2        -122.24   3.521  \n",
       "3        -122.25   3.413  \n",
       "4        -122.25   3.422  \n",
       "...          ...     ...  \n",
       "13755    -117.16   1.172  \n",
       "13756    -117.16   0.943  \n",
       "13757    -117.14   2.696  \n",
       "13758    -117.15   2.372  \n",
       "13759    -117.15   2.171  \n",
       "\n",
       "[13760 rows x 9 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f7111cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13760, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show data\n",
    "# m = number of training examples\n",
    "m = train_df.values.shape[0]\n",
    "# n = number of features\n",
    "n = len(train_df.drop(columns='target').columns)\n",
    "\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc29288c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13760, 8), (13760, 1))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X should be of the dimensions m, n\n",
    "\n",
    "X = train_df.drop(columns='target').values.reshape(m,n)\n",
    "Y = train_df['target'].values.reshape(m,1)\n",
    "\n",
    "X.shape, Y.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2a8d40d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n",
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n",
      "g_wb shape (), prediction: 459.9999976194083\n",
      "all tests passed\n"
     ]
    }
   ],
   "source": [
    "# example from machine learning specialisation\n",
    "\n",
    "\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p\n",
    "\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")\n",
    "\n",
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "\n",
    "def forward_prop(X, w, b):\n",
    "    n = X.shape[0]\n",
    "    # reshape step important for later functions\n",
    "    Y_hat = np.dot(X_train, w_init) + b_init\n",
    "    return Y_hat\n",
    "\n",
    "# ml spec function  prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")\n",
    "\n",
    "# shows that prediction value was the same for for loop as for cevotrised implementation ( in my code)\n",
    "# this shows my code is right currently\n",
    "g_wb = forward_prop(X_train, w_init, b_init)[0]\n",
    "print(f\"g_wb shape {g_wb.shape}, prediction: {g_wb}\")\n",
    "\n",
    "assert g_wb == f_wb\n",
    "\n",
    "if g_wb == f_wb:\n",
    "    print('all tests passed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "23c7b135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459.9999976194083"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.dot(X_train, w_init) + b_init)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ee8c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904045996674e-12 <class 'numpy.float64'>\n",
      "Cost at optimal w : 1.5578904045996674e-12 <class 'numpy.float64'>\n",
      "Cost at optimal w : 1.5578904045996674e-12 <class 'numpy.float64'>\n",
      "all tests passed\n"
     ]
    }
   ],
   "source": [
    "# compute cost code comparisons\n",
    "\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "\n",
    "def ml_compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost\n",
    "\n",
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "ml_cost = ml_compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {ml_cost}', type(ml_cost))\n",
    "\n",
    "\n",
    "# correct, vectorised implementation of the cots function\n",
    "m = X_train.shape[0]\n",
    "Y_hat = np.dot(X_train, w_init) + b_init\n",
    "cost = np.sum((Y_hat - y_train)**2 ) / (2*m)\n",
    "\n",
    "print(f'Cost at optimal w : {cost}', type(cost))\n",
    "\n",
    "# function version of vectorised implementation\n",
    "def calculate_cost(X, Y_train, w, b):\n",
    "    m = X_train.shape[0]\n",
    "    Y_hat = forward_prop(X_train, w, b)\n",
    "    cost = np.sum((Y_hat - y_train)**2 ) / (2*m)\n",
    "    return cost\n",
    "\n",
    "f_cost = calculate_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {f_cost}', type(f_cost))\n",
    "\n",
    "assert f_cost == ml_cost\n",
    "\n",
    "if f_cost == ml_cost:\n",
    "    print('all tests passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c9caed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.6739251122999121e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n",
      "dw at initial w,b: \n",
      " -1.6739251122999121e-06\n",
      "db at initial w,b: [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n",
      "f_dw at initial w,b: \n",
      " -1.6739251122999121e-06\n",
      "f_db at initial w,b: [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n",
      "all tests passed\n"
     ]
    }
   ],
   "source": [
    "def ml_compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    \n",
    "    # for each training and each feature example calculate the derivitives then....\n",
    "    # take the average accross number of training examples\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        # for each feature\n",
    "        for j in range(n):\n",
    "            # dw = 0 + y_hat - y * X\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]   \n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "# resetting variables\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "m,n = X_train.shape           #(number of examples, number of features)\n",
    "\n",
    "\n",
    "# ml_spec compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = ml_compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')\n",
    "\n",
    "dw = 0 \n",
    "db = 0\n",
    "\n",
    "Y_hat = forward_prop(X_train, w_init, b_init)\n",
    "\n",
    "db = np.mean(Y_hat - y_train)\n",
    "dw = np.sum(((Y_hat - y_train) * X_train.T), axis=1) / m\n",
    "print(f'dw at initial w,b: \\n {db}')\n",
    "print(f'db at initial w,b: {dw}')\n",
    "\n",
    "def calculate_grads(X_train, Y_train, w, b):\n",
    "    m, n = X_train.shape\n",
    "    Y_hat = forward_prop(X_train, w, b)\n",
    "    db = np.mean(Y_hat - Y_train)\n",
    "    dw = np.sum(((Y_hat - Y_train) * X_train.T), axis=1) / m\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "f_dw, f_db = calculate_grads(X_train, y_train, w_init, b_init)\n",
    "print(f'f_dw at initial w,b: \\n {f_db}')\n",
    "print(f'f_db at initial w,b: {f_dw}')\n",
    "\n",
    "assert f_dw[0] == tmp_dj_dw[0]\n",
    "assert f_db == tmp_dj_db\n",
    "\n",
    "if  f_dw[0] == tmp_dj_dw[0]:\n",
    "    print('all tests passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d36305",
   "metadata": {},
   "source": [
    "grad returning incorrect still on below function despite above tests passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8fa76b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "        \n",
    "#         print(f'dj_db on iter {i}:',dj_db)\n",
    "#         print(f'dj_dw on iter {i}:',dj_dw)\n",
    "            \n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "#         if i ==2:\n",
    "#             break\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a089e25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b,w found by gradient descent: -0.00,[ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    ml_compute_cost, ml_compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5ae0dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, compute_cost, compute_grads, learning_rate, num_iters):\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # calculate gradient & updte params\n",
    "        dw, db = calculate_grads(X, y, w, b)\n",
    "\n",
    "#         print(f'db on iter {i}:',db)\n",
    "#         print(f'dw on iter {i}:',dw)\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        if i < 100000: \n",
    "            J_history.append(compute_cost(X, y, w, b))\n",
    "\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "#         if i ==2:\n",
    "#             break    \n",
    "        \n",
    "        return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a70c3ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.00   \n",
      "b,w found by gradient descent: 0.00,[1.36311787e-09 3.13598628e-12 1.10872787e-12 3.46201689e-11] \n",
      "prediction: 0.00, target value: 460\n",
      "prediction: 0.00, target value: 232\n",
      "prediction: 0.00, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    calculate_cost, calculate_grads, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12745122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d5ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "w, b = initial_rand(X_train)\n",
    "# print('w',w,'b',b)\n",
    "\n",
    "Y_hat = forward_prop(X_train, w, b)[0]\n",
    "# print('yhat',y_hat.shape)\n",
    "Y_hat\n",
    "\n",
    "w, b, J_history = single_var_backprop(X_train, y_train, w_init, b_init, Y_hat)\n",
    "# print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51357a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3bfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce590f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd7aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245352f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
